{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'pogusthewhisper'\n",
    "os.environ['KAGGLE_KEY'] = '755720e5147a6550b4d67a3b66981cb4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!kaggle kernels output pogusthewhisper/alb-classify-dataset -p /kaggle/working/\n",
    "!unzip -q /kaggle/working/dataset.zip\n",
    "!mv /kaggle/working/kaggle/working/dataset /kaggle/working/\n",
    "!rm -r /kaggle/working/kaggle && rm /kaggle/working/surgicare-dataset.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:29.478290Z",
     "iopub.status.busy": "2025-06-07T16:56:29.478047Z",
     "iopub.status.idle": "2025-06-07T16:56:29.493530Z",
     "shell.execute_reply": "2025-06-07T16:56:29.492750Z",
     "shell.execute_reply.started": "2025-06-07T16:56:29.478274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T17:22:32.859615Z",
     "iopub.status.busy": "2025-06-07T17:22:32.859070Z",
     "iopub.status.idle": "2025-06-07T17:22:32.865145Z",
     "shell.execute_reply": "2025-06-07T17:22:32.864173Z",
     "shell.execute_reply.started": "2025-06-07T17:22:32.859591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/kaggle/working/dataset/train'\n",
    "test_dir = '/kaggle/working/dataset/test'\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:29.641848Z",
     "iopub.status.busy": "2025-06-07T16:56:29.641564Z",
     "iopub.status.idle": "2025-06-07T16:56:29.654716Z",
     "shell.execute_reply": "2025-06-07T16:56:29.654117Z",
     "shell.execute_reply.started": "2025-06-07T16:56:29.641830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WoundClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        base = efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.DEFAULT)\n",
    "        n_features = base.classifier[1].in_features\n",
    "        base.classifier = nn.Identity()\n",
    "        self.backbone = base\n",
    "\n",
    "        self.shared_head = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.layer_groups = [\n",
    "            self.backbone.features[0:2],\n",
    "            self.backbone.features[2:4],\n",
    "            self.backbone.features[4:6],\n",
    "            self.backbone.features[6:]\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.shared_head(x)\n",
    "        cls_out = self.class_head(x)\n",
    "        return cls_out\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_all(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_group(self, group_idx):\n",
    "        for param in self.layer_groups[group_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_group(self, group_idx):\n",
    "        for param in self.layer_groups[group_idx].parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:30.184305Z",
     "iopub.status.busy": "2025-06-07T16:56:30.183801Z",
     "iopub.status.idle": "2025-06-07T16:56:30.190396Z",
     "shell.execute_reply": "2025-06-07T16:56:30.189669Z",
     "shell.execute_reply.started": "2025-06-07T16:56:30.184283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLossWithSmoothing(nn.Module):\n",
    "    def __init__(self, gamma=2.0, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        focal = (1 - probs.gather(1, targets.unsqueeze(1)).squeeze(1)) ** self.gamma\n",
    "        loss = -torch.sum(true_dist * log_probs, dim=1)\n",
    "        return (focal * loss).mean()\n",
    "\n",
    "class ClassificationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cls_loss = FocalLossWithSmoothing()\n",
    "\n",
    "    def forward(self, cls_pred, cls_target):\n",
    "        loss_cls = self.cls_loss(cls_pred, cls_target)\n",
    "        return loss_cls, loss_cls.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:30.763064Z",
     "iopub.status.busy": "2025-06-07T16:56:30.762799Z",
     "iopub.status.idle": "2025-06-07T16:56:30.768271Z",
     "shell.execute_reply": "2025-06-07T16:56:30.767449Z",
     "shell.execute_reply.started": "2025-06-07T16:56:30.763047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=6, delta=0.0, save_path='best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None or score > self.best_score + self.delta:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:31.121836Z",
     "iopub.status.busy": "2025-06-07T16:56:31.121260Z",
     "iopub.status.idle": "2025-06-07T16:56:31.129444Z",
     "shell.execute_reply": "2025-06-07T16:56:31.128634Z",
     "shell.execute_reply.started": "2025-06-07T16:56:31.121815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset, DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def create_sampler(subset):\n",
    "    labels = [subset.dataset.targets[i] for i in subset.indices]\n",
    "    class_counts = Counter(labels)\n",
    "    class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "    weights = [class_weights[label] for label in labels]\n",
    "    return WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "def get_kfold_dataloaders(dataset, k=5, batch_size=32, val_split=0.2):\n",
    "    y = dataset.targets\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    folds = []\n",
    "\n",
    "    for train_val_idx, _ in skf.split(np.arange(len(dataset)), y):\n",
    "        y_train_val = [y[i] for i in train_val_idx]\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=val_split, random_state=42)\n",
    "        train_idx, val_idx = next(sss.split(train_val_idx, y_train_val))\n",
    "\n",
    "        train_indices = [train_val_idx[i] for i in train_idx]\n",
    "        val_indices = [train_val_idx[i] for i in val_idx]\n",
    "\n",
    "        train_ds = Subset(dataset, train_indices)\n",
    "        val_ds = Subset(dataset, val_indices)\n",
    "        train_ds.dataset.transform = train_tf\n",
    "        val_ds.dataset.transform = val_tf\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=create_sampler(train_ds))\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "        folds.append({'train': train_loader, 'val': val_loader})\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:31.535245Z",
     "iopub.status.busy": "2025-06-07T16:56:31.534967Z",
     "iopub.status.idle": "2025-06-07T16:56:31.549632Z",
     "shell.execute_reply": "2025-06-07T16:56:31.548825Z",
     "shell.execute_reply.started": "2025-06-07T16:56:31.535223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_top_down(model_class, dataset, num_classes=5, stages=4, epochs_per_stage=10,\n",
    "                   batch_size=16, base_lr=5e-4, weight_decay=5e-5,\n",
    "                   patience=5, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "\n",
    "    folds = get_kfold_dataloaders(dataset, k=5, batch_size=batch_size)\n",
    "    results = []\n",
    "\n",
    "    for fold_idx, loaders in enumerate(folds):\n",
    "        print(f\"\\nStarting Fold {fold_idx + 1}/5\")\n",
    "\n",
    "        model = model_class(num_classes=num_classes).to(device)\n",
    "        model.freeze_all()\n",
    "        model.shared_head.requires_grad_(True)\n",
    "        model.class_head.requires_grad_(True)\n",
    "\n",
    "        train_loader = loaders['train']\n",
    "        val_loader = loaders['val']\n",
    "        loss_fn = ClassificationLoss()\n",
    "\n",
    "        for stage in range(stages):\n",
    "            print(f\"\\nStage {stage + 1}/{stages}: Unfreezing group {stage}\")\n",
    "            if stage > 0:\n",
    "                model.unfreeze_group(stage - 1)\n",
    "\n",
    "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                          lr=base_lr * (0.5 ** stage), weight_decay=weight_decay)\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=epochs_per_stage)\n",
    "            scaler = GradScaler()\n",
    "            early_stopper = EarlyStopping(patience=patience,\n",
    "                                          save_path=f'topdown_model_fold{fold_idx}_stage{stage}.pt')\n",
    "\n",
    "            for epoch in range(epochs_per_stage):\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "\n",
    "                for inputs, targets in train_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    cls_targets = targets.to(device).long()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    with autocast(device_type=device):\n",
    "                        cls_out = model(inputs)\n",
    "                        loss, _ = loss_fn(cls_out, cls_targets)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                train_loss /= len(train_loader.dataset)\n",
    "\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                all_preds, all_targets = [], []\n",
    "                with torch.no_grad():\n",
    "                    for inputs, targets in val_loader:\n",
    "                        inputs = inputs.to(device)\n",
    "                        cls_targets = targets.to(device).long()\n",
    "\n",
    "                        cls_out = model(inputs)\n",
    "                        loss, _ = loss_fn(cls_out, cls_targets)\n",
    "                        val_loss += loss.item() * inputs.size(0)\n",
    "                        preds = torch.argmax(cls_out, dim=1)\n",
    "                        all_preds.extend(preds.cpu().numpy())\n",
    "                        all_targets.extend(cls_targets.cpu().numpy())\n",
    "\n",
    "                val_loss /= len(val_loader.dataset)\n",
    "                val_acc = accuracy_score(all_targets, all_preds)\n",
    "                val_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "                scheduler.step()\n",
    "\n",
    "                print(f\"[Fold {fold_idx+1} | Stage {stage+1} | Epoch {epoch+1}/{epochs_per_stage}] \"\n",
    "                      f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} \"\n",
    "                      f\"| Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "                early_stopper(val_loss, model)\n",
    "                if early_stopper.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "            model.load_state_dict(torch.load(f'topdown_model_fold{fold_idx}_stage{stage}.pt'))\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                cls_targets = targets.to(device).long()\n",
    "\n",
    "                cls_out = model(inputs)\n",
    "                preds = torch.argmax(cls_out, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(cls_targets.cpu().numpy())\n",
    "\n",
    "        fold_acc = accuracy_score(all_targets, all_preds)\n",
    "        fold_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "        results.append({'fold': fold_idx + 1, 'accuracy': fold_acc, 'f1': fold_f1})\n",
    "        print(f\"Fold {fold_idx + 1} Accuracy: {fold_acc:.4f}, F1: {fold_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    for res in results:\n",
    "        print(f\"Fold {res['fold']} - Accuracy: {res['accuracy']:.4f}, F1: {res['f1']:.4f}\")\n",
    "    print(f\"\\nAvg Accuracy: {np.mean([r['accuracy'] for r in results]):.4f} | \"\n",
    "          f\"Avg F1: {np.mean([r['f1'] for r in results]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-07T16:56:31.908203Z",
     "iopub.status.busy": "2025-06-07T16:56:31.907758Z",
     "iopub.status.idle": "2025-06-07T17:07:37.155798Z",
     "shell.execute_reply": "2025-06-07T17:07:37.154937Z",
     "shell.execute_reply.started": "2025-06-07T16:56:31.908181Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = datasets.ImageFolder(train_dir)\n",
    "train_top_down(WoundClassifier, dataset, num_classes=len(dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T17:08:16.664379Z",
     "iopub.status.busy": "2025-06-07T17:08:16.663912Z",
     "iopub.status.idle": "2025-06-07T17:08:16.672609Z",
     "shell.execute_reply": "2025-06-07T17:08:16.671898Z",
     "shell.execute_reply.started": "2025-06-07T17:08:16.664345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names=None, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_targets, all_preds,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    for idx, acc in enumerate(per_class_acc):\n",
    "        name = class_names[idx] if class_names else str(idx)\n",
    "        print(f\"Accuracy for class '{name}': {acc:.2%}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': np.mean(all_preds == all_targets),\n",
    "        'f1_macro': f1_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "        'per_class_accuracy': per_class_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T17:14:11.958924Z",
     "iopub.status.busy": "2025-06-07T17:14:11.958476Z",
     "iopub.status.idle": "2025-06-07T17:14:11.965746Z",
     "shell.execute_reply": "2025-06-07T17:14:11.964913Z",
     "shell.execute_reply.started": "2025-06-07T17:14:11.958899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_all_folds(model_class, dataset, stages=4, device='cuda'):\n",
    "    class_names = dataset.classes\n",
    "    folds = get_kfold_dataloaders(dataset, k=5, batch_size=32)\n",
    "    metrics_all = []\n",
    "\n",
    "    for fold_idx, loaders in enumerate(folds):\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"📁 Evaluating Fold {fold_idx + 1}\")\n",
    "        print(f\"============================\")\n",
    "\n",
    "        model = model_class(num_classes=len(class_names)).to(device)\n",
    "        model.load_state_dict(torch.load(f'topdown_model_fold{fold_idx}_stage{stages - 1}.pt'))\n",
    "        model.eval()\n",
    "\n",
    "        fold_metrics = evaluate_model(model, loaders['val'], class_names=class_names, device=device)\n",
    "        metrics_all.append(fold_metrics)\n",
    "\n",
    "        print(f\"Fold {fold_idx + 1} Accuracy: {fold_metrics['accuracy']:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean([m['accuracy'] for m in metrics_all])\n",
    "    avg_f1 = np.mean([m['f1_macro'] for m in metrics_all])\n",
    "    print(f\"\\nAvg Accuracy: {avg_acc:.4f} | Avg F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "    return metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T17:23:49.018948Z",
     "iopub.status.busy": "2025-06-07T17:23:49.018178Z",
     "iopub.status.idle": "2025-06-07T17:24:11.215092Z",
     "shell.execute_reply": "2025-06-07T17:24:11.214445Z",
     "shell.execute_reply.started": "2025-06-07T17:23:49.018923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(test_dir, transform=val_tf)\n",
    "evaluate_all_folds(WoundClassifier, test_dataset, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
