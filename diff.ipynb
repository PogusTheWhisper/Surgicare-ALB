{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0889b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes not found, CUDA-based quantization will not be available\n",
      "PyTorch version: 2.8.0.dev20250412\n",
      "MPS available: True\n",
      "MPS built: True\n",
      "Using MPS\n",
      "Loading models...\n",
      "MPS quantization enabled: custom 8-bit + FP16\n",
      "Loading BLIP model...\n",
      "Applying MPS quantization to BLIP model...\n",
      "Loading VAE...\n",
      "Applying MPS quantization to VAE...\n",
      "Downloading COSXL model...\n",
      "Creating pipeline on mps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f209ab4b0a5b4bd8b870b6acb76433a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13ab43b6e8140c1b893736c9ee5f9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying MPS quantization to pipeline...\n",
      "All models loaded\n",
      "Starting Gradio interface with device: mps\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating caption...\n",
      "Error generating caption: Tensor for argument weight is on cpu but expected on mps\n",
      "Generating caption...\n",
      "Error generating caption: Tensor for argument weight is on cpu but expected on mps\n",
      "Using prompt: 'red belt'\n",
      "Error during image generation: Tensor for argument weight is on cpu but expected on mps\n",
      "Using prompt: 'red belt'\n",
      "Error during image generation: Tensor for argument weight is on cpu but expected on mps\n",
      "Using prompt: 'red belt'\n",
      "Error during image generation: Tensor for argument weight is on cpu but expected on mps\n",
      "Using prompt: 'red belt'\n",
      "Error during image generation: Tensor for argument weight is on cpu but expected on mps\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import gradio as gr\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from diffusers import (\n",
    "    StableDiffusionXLInstructPix2PixPipeline,\n",
    "    EDMEulerScheduler,\n",
    "    AutoencoderKL,\n",
    ")\n",
    "from huggingface_hub import hf_hub_download\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add support for quantization\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "    HAS_BNB = True\n",
    "except ImportError:\n",
    "    HAS_BNB = False\n",
    "    print(\"bitsandbytes not found, CUDA-based quantization will not be available\")\n",
    "\n",
    "# ========== BASIC MPS SETUP ==========\n",
    "# Set environment variables\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Check device availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# Set device - simpler approach\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ========== Memory Management ==========\n",
    "def torch_gc():\n",
    "    \"\"\"Basic memory cleanup compatible with all PyTorch versions\"\"\"\n",
    "    try:\n",
    "        # Try to empty MPS cache if available\n",
    "        if device == \"mps\" and hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Always do GC collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Try CUDA cleanup just in case\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ========== Custom Quantization for MPS ==========\n",
    "class MPSQuantizedLinear(torch.nn.Module):\n",
    "    \"\"\"MPS-friendly quantized linear layer using 8-bit representation\"\"\"\n",
    "    def __init__(self, linear_layer):\n",
    "        super().__init__()\n",
    "        self.input_size = linear_layer.in_features\n",
    "        self.output_size = linear_layer.out_features\n",
    "        \n",
    "        # Quantize weights to int8, scale to preserve range\n",
    "        weight = linear_layer.weight.data.float()\n",
    "        self.w_scale = weight.abs().max() / 127.0\n",
    "        self.w_quant = (weight / self.w_scale).round().char()\n",
    "        \n",
    "        # Keep bias in float32\n",
    "        self.bias = None\n",
    "        if linear_layer.bias is not None:\n",
    "            self.bias = linear_layer.bias.data.clone()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Convert quantized weights back to float for computation\n",
    "        w_dequant = self.w_quant.float() * self.w_scale\n",
    "        output = torch.nn.functional.linear(x, w_dequant)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "\n",
    "def quantize_module_for_mps(module, dtype=torch.float16):\n",
    "    \"\"\"Apply MPS-friendly quantization or precision reduction to modules recursively\"\"\"\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            setattr(module, name, MPSQuantizedLinear(child))\n",
    "        elif len(list(child.children())) > 0:\n",
    "            # Recursively quantize child modules\n",
    "            quantize_module_for_mps(child, dtype)\n",
    "    \n",
    "    # Convert remaining parameters to half precision\n",
    "    for param_name, param in module.named_parameters():\n",
    "        if param.dtype == torch.float32:\n",
    "            param.data = param.data.to(dtype)\n",
    "    \n",
    "    return module\n",
    "\n",
    "# ========== Padding Helper ==========\n",
    "def pad_to_square_and_record(image, fill_color=(255, 255, 255)):\n",
    "    width, height = image.size\n",
    "    max_side = max(width, height)\n",
    "    max_side = (max_side + 7) // 8 * 8  # Make divisible by 8\n",
    "\n",
    "    delta_w = max_side - width\n",
    "    delta_h = max_side - height\n",
    "    padding = (\n",
    "        delta_w // 2, delta_h // 2,\n",
    "        delta_w - delta_w // 2, delta_h - delta_h // 2\n",
    "    )\n",
    "    padded = ImageOps.expand(image, padding, fill=fill_color)\n",
    "    return padded, padding\n",
    "\n",
    "def crop_back_to_original(image: Image.Image, padding) -> Image.Image:\n",
    "    left, top, right, bottom = padding\n",
    "    width, height = image.size\n",
    "    return image.crop((left, top, width - right, height - bottom))\n",
    "\n",
    "def remove_white_background(image: Image.Image, white_thresh=240) -> Image.Image:\n",
    "    image = image.convert(\"RGBA\")\n",
    "    data = np.array(image)\n",
    "\n",
    "    r, g, b, a = data.T\n",
    "    white_mask = (r >= white_thresh) & (g >= white_thresh) & (b >= white_thresh)\n",
    "    data[..., 3][white_mask.T] = 0\n",
    "\n",
    "    fade_mask = (r >= 200) & (g >= 200) & (b >= 200) & (~white_mask)\n",
    "    data[..., 3][fade_mask.T] = 128\n",
    "    return Image.fromarray(data)\n",
    "\n",
    "# ========== Load Models ==========\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# Configure quantization based on device\n",
    "quantization_config = None\n",
    "quantization_enabled = False\n",
    "mps_quantization = False\n",
    "\n",
    "# For CUDA devices, use BnB 4-bit quantization\n",
    "if device == \"cuda\" and HAS_BNB:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",  # Q4_K is equivalent to NF4 (normalized float 4)\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    quantization_enabled = True\n",
    "    print(\"CUDA quantization config enabled: q4km (4-bit quantization)\")\n",
    "# For MPS devices, use custom 8-bit quantization\n",
    "elif device == \"mps\":\n",
    "    mps_quantization = True\n",
    "    dtype = torch.float16  # Use half precision on MPS\n",
    "    quantization_enabled = True\n",
    "    print(\"MPS quantization enabled: custom 8-bit + FP16\")\n",
    "else:\n",
    "    # CPU or other devices\n",
    "    dtype = torch.float32\n",
    "    print(\"Quantization not available, loading full precision models\")\n",
    "\n",
    "# Load BLIP for captioning\n",
    "print(\"Loading BLIP model...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\", \n",
    "    use_fast=False\n",
    ")\n",
    "torch_gc()\n",
    "\n",
    "# Load BLIP model with appropriate quantization\n",
    "if device == \"cuda\" and HAS_BNB and quantization_config:\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\",\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "else:\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\"\n",
    "    )\n",
    "    \n",
    "    # Apply MPS quantization if needed\n",
    "    if mps_quantization:\n",
    "        print(\"Applying MPS quantization to BLIP model...\")\n",
    "        blip_model = quantize_module_for_mps(blip_model, dtype=dtype)\n",
    "\n",
    "blip_model.to(device)\n",
    "torch_gc()\n",
    "\n",
    "# Load VAE\n",
    "print(\"Loading VAE...\")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\", \n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "# Apply MPS quantization to VAE if needed\n",
    "if mps_quantization:\n",
    "    print(\"Applying MPS quantization to VAE...\")\n",
    "    vae = quantize_module_for_mps(vae, dtype=dtype)\n",
    "\n",
    "torch_gc()\n",
    "\n",
    "# Download COSXL model\n",
    "print(\"Downloading COSXL model...\")\n",
    "cosxl_edit_path = hf_hub_download(repo_id=\"stabilityai/cosxl\", filename=\"cosxl_edit.safetensors\")\n",
    "torch_gc()\n",
    "\n",
    "# Create pipeline with appropriate quantization\n",
    "print(f\"Creating pipeline on {device}...\")\n",
    "if device == \"cuda\" and HAS_BNB and quantization_config:\n",
    "    pipe = StableDiffusionXLInstructPix2PixPipeline.from_single_file(\n",
    "        cosxl_edit_path,\n",
    "        num_in_channels=8,\n",
    "        is_cosxl_edit=True,\n",
    "        vae=vae,\n",
    "        torch_dtype=dtype,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "else:\n",
    "    pipe = StableDiffusionXLInstructPix2PixPipeline.from_single_file(\n",
    "        cosxl_edit_path,\n",
    "        num_in_channels=8,\n",
    "        is_cosxl_edit=True,\n",
    "        vae=vae,\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    \n",
    "    # Apply MPS quantization if needed\n",
    "    if mps_quantization:\n",
    "        print(\"Applying MPS quantization to pipeline...\")\n",
    "        # Quantize UNet and text encoder\n",
    "        pipe.unet = quantize_module_for_mps(pipe.unet, dtype=dtype)\n",
    "        pipe.text_encoder = quantize_module_for_mps(pipe.text_encoder, dtype=dtype)\n",
    "        pipe.text_encoder_2 = quantize_module_for_mps(pipe.text_encoder_2, dtype=dtype)\n",
    "\n",
    "# Move to device\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Set scheduler\n",
    "pipe.scheduler = EDMEulerScheduler(\n",
    "    sigma_min=0.002,\n",
    "    sigma_max=120.0,\n",
    "    sigma_data=1.0,\n",
    "    prediction_type=\"v_prediction\",\n",
    "    sigma_schedule=\"exponential\"\n",
    ")\n",
    "\n",
    "# Enable memory efficient attention if available\n",
    "if hasattr(pipe, \"enable_attention_slicing\"):\n",
    "    pipe.enable_attention_slicing(1)\n",
    "\n",
    "# Enable memory efficient attention for CUDA\n",
    "if hasattr(pipe, \"enable_xformers_memory_efficient_attention\") and device == \"cuda\":\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "torch_gc()\n",
    "\n",
    "# Skip warm-up as it might be causing issues\n",
    "print(\"All models loaded\")\n",
    "\n",
    "# ========== Prompt Generation ==========\n",
    "def generate_prompt(image: Image.Image):\n",
    "    try:\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "        # Resize to save memory\n",
    "        max_size = 512\n",
    "        width, height = image.size\n",
    "        if max(width, height) > max_size:\n",
    "            ratio = max_size / max(width, height)\n",
    "            new_width = int(width * ratio)\n",
    "            new_height = int(height * ratio)\n",
    "            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Process image\n",
    "        inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate caption\n",
    "        with torch.no_grad():\n",
    "            output = blip_model.generate(**inputs, max_length=30)\n",
    "        \n",
    "        caption = blip_processor.decode(output[0], skip_special_tokens=True)\n",
    "        torch_gc()\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption: {e}\")\n",
    "        return \"An image\"\n",
    "\n",
    "def suggest_prompt_only(image):\n",
    "    if image is None:\n",
    "        return \"\"\n",
    "        \n",
    "    print(\"Generating caption...\")\n",
    "    return generate_prompt(image)\n",
    "\n",
    "# ========== Core Generation ==========\n",
    "def edit_image(input_img, instruction_override, cached_prompt, cfg_scale=7.0, steps=25):\n",
    "    if input_img is None:\n",
    "        return None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    input_img = input_img.convert(\"RGB\")\n",
    "    original_size = input_img.size\n",
    "    \n",
    "    # Preprocess image\n",
    "    padded_img, padding = pad_to_square_and_record(input_img)\n",
    "    \n",
    "    # Use standard size for better results\n",
    "    resize_size = 512\n",
    "    padded_img = padded_img.resize((resize_size, resize_size), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Construct prompt\n",
    "    final_prompt = instruction_override.strip() if instruction_override.strip() else f\"{cached_prompt}, but modified\"\n",
    "    print(f\"Using prompt: '{final_prompt}'\")\n",
    "\n",
    "    # Clean memory\n",
    "    torch_gc()\n",
    "    \n",
    "    try:\n",
    "        # Ensure model is on correct device\n",
    "        pipe.to(device)\n",
    "        \n",
    "        # Run inference with reduced steps for MPS if needed\n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=final_prompt,\n",
    "                image=padded_img,\n",
    "                height=resize_size,\n",
    "                width=resize_size,\n",
    "                guidance_scale=cfg_scale,\n",
    "                num_inference_steps=steps\n",
    "            ).images[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image generation: {e}\")\n",
    "        return input_img\n",
    "    finally:\n",
    "        torch_gc()\n",
    "\n",
    "    # Postprocess result\n",
    "    result = remove_white_background(result)\n",
    "    result = crop_back_to_original(result, padding)\n",
    "    result = result.resize(original_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ========== Gradio UI ==========\n",
    "with gr.Blocks() as demo:\n",
    "    quant_description = \"\"\n",
    "    if device == \"cuda\" and HAS_BNB and quantization_enabled:\n",
    "        quant_description = \"with Q4KM (4-bit) quantization\"\n",
    "    elif device == \"mps\" and quantization_enabled:\n",
    "        quant_description = \"with custom 8-bit + FP16 quantization\"\n",
    "    else:\n",
    "        quant_description = \"in full precision\"\n",
    "        \n",
    "    gr.Markdown(f\"## ✨ COSXL Edit with Quantization Support\")\n",
    "    gr.Markdown(f\"Running on: **{device.upper()}** {quant_description}\")\n",
    "\n",
    "    with gr.Row():\n",
    "        input_image = gr.Image(type=\"pil\", label=\"Upload Reference Image\")\n",
    "        output_image = gr.Image(type=\"pil\", label=\"Edited Output\")\n",
    "\n",
    "    with gr.Row():\n",
    "        suggested_prompt = gr.Textbox(label=\"🧠 Auto-Suggested Prompt\", interactive=False)\n",
    "        user_instruction = gr.Textbox(label=\"✍️ Your Edit Instruction\", placeholder=\"e.g. make it futuristic\")\n",
    "\n",
    "    with gr.Row():\n",
    "        slider_cfg = gr.Slider(1.0, 15.0, value=7.0, step=0.5, label=\"Guidance Scale (higher = more adherence to prompt)\")\n",
    "        slider_steps = gr.Slider(10, 50, value=25, step=1, label=\"Inference Steps (higher = more quality)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        run_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "        clear_btn = gr.Button(\"Clear\")\n",
    "        download_btn = gr.File(label=\"📥 Download as PNG\", interactive=False)\n",
    "        \n",
    "    # Quantization options\n",
    "    with gr.Accordion(\"Quantization Details\", open=False):\n",
    "        if device == \"cuda\" and HAS_BNB and quantization_enabled:\n",
    "            quantization_info = gr.Markdown(f\"\"\"\n",
    "            ### 4-bit Quantization (Q4KM/NF4)\n",
    "            \n",
    "            - Type: NF4 (Normalized Float 4-bit, equivalent to Q4_K/Q4KM)\n",
    "            - Method: 4-bit quantization with double quantization\n",
    "            - Compute Dtype: FP16\n",
    "            - Memory savings: ~75% compared to FP16\n",
    "            \"\"\")\n",
    "        elif device == \"mps\" and quantization_enabled:\n",
    "            quantization_info = gr.Markdown(f\"\"\"\n",
    "            ### Custom MPS Quantization\n",
    "            \n",
    "            - Weights: 8-bit integer quantization\n",
    "            - Activations: FP16 (half precision)\n",
    "            - Method: Custom linear layer implementation for MPS\n",
    "            - Memory savings: ~60% compared to FP32\n",
    "            - Performance: Slightly slower but more memory efficient\n",
    "            \"\"\")\n",
    "        else:\n",
    "            quantization_info = gr.Markdown(f\"\"\"\n",
    "            ### Full Precision Mode\n",
    "            \n",
    "            Quantization is not enabled. Models are running in full precision ({dtype}).\n",
    "            \"\"\")\n",
    "        \n",
    "    # System info\n",
    "    with gr.Accordion(\"System Info\", open=False):\n",
    "        gr.Markdown(f\"\"\"\n",
    "        - Device: {device.upper()}\n",
    "        - PyTorch: {torch.__version__}\n",
    "        - MPS Available: {torch.backends.mps.is_available()}\n",
    "        - MPS Built: {torch.backends.mps.is_built()}\n",
    "        - BnB Available: {HAS_BNB}\n",
    "        - Quantization Enabled: {quantization_enabled}\n",
    "        - Quantization Mode: {\"CUDA Q4KM\" if device == \"cuda\" and HAS_BNB and quantization_enabled else \"MPS 8-bit\" if device == \"mps\" and quantization_enabled else \"None\"}\n",
    "        \"\"\")\n",
    "\n",
    "    # Tips for MPS users\n",
    "    if device == \"mps\":\n",
    "        with gr.Accordion(\"Tips for Apple Silicon Users\", open=True):\n",
    "            gr.Markdown(f\"\"\"\n",
    "            ### Optimizing Performance on Apple Silicon\n",
    "            \n",
    "            - Lower the inference steps (15-20) for faster generation\n",
    "            - If you encounter out-of-memory errors, restart the application and try again\n",
    "            - Close memory-intensive applications when running\n",
    "            - If you need more VRAM, restart your computer to clear the memory\n",
    "            - Smaller images (512x512) work better than larger ones\n",
    "            \"\"\")\n",
    "\n",
    "    # Event handlers\n",
    "    input_image.change(fn=suggest_prompt_only, inputs=input_image, outputs=suggested_prompt)\n",
    "    \n",
    "    def clear_outputs():\n",
    "        return None, None, \"\"\n",
    "    \n",
    "    clear_btn.click(fn=clear_outputs, inputs=[], outputs=[input_image, output_image, suggested_prompt])\n",
    "\n",
    "    def process_and_save(input_img, instruction_override, cached_prompt, cfg_scale, steps):\n",
    "        if input_img is None:\n",
    "            return None, None\n",
    "        \n",
    "        result = edit_image(input_img, instruction_override, cached_prompt, cfg_scale, steps)\n",
    "        if result is None:\n",
    "            return None, None\n",
    "            \n",
    "        path = \"edited_result.png\"\n",
    "        result.save(path, format=\"PNG\")\n",
    "        return result, path\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=process_and_save,\n",
    "        inputs=[input_image, user_instruction, suggested_prompt, slider_cfg, slider_steps],\n",
    "        outputs=[output_image, download_btn]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Gradio interface with device: {device}\")\n",
    "    demo.launch(\n",
    "        debug=True, \n",
    "        server_name=\"0.0.0.0\", \n",
    "        share=False,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1226cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
